---
title: 学不会的数学
date: 2024-05-06 21:23:23
mathjax: true
top: 70
categories: # 分类
- Math
tags: # 标签
- 数学知识
---


# 线性代数

<!--more-->

## 对称矩阵

矩阵乘以自身的转置必为对称矩阵，证明过程为

$$
(A A^{T})^{T} = (A^{T})^{T} A^{T} = A A^{T}
$$

## 矩阵的秩

矩阵的秩（Rank）是指矩阵中线性独立行或列的最大数目。

### 矩阵秩的性质

1. rank($A$) = rank($A^{T}$) = rank($A A^{T}$) = rank($A^{T} A$)\
用非公式、通俗易懂的语言解释一下行秩等于列秩。直观的解释就是当某个列向量开始与其他列向量线性相关时，说明这个列向量可以用其他向量线性表示，其实也就是它没有跑到其他向量张成的线性空间之外。既然这个列向量可以用其他列向量线性表示，从行向量上来看，这个分量的添加无非相当于给行向量添加了一个没有用的分量，因为行向量不需要这个分量也可以被完全描述，所以这个分量的增加自然也不会改变行向量的秩。正如同一维空间二维空间不需要用三维向量来描述一样，非要用三维向量来描述一维空间也必然有两个维度是多余的，因为他足以被一个维度唯一确定。反之，如果某个列向量与其他向量线性无关，对于行向量来说，没有这个分量必然导致已有分量不能完全描述行向量，所以这个分量的增加必然改变行向量的秩。代数的证明很简单，就是不断地进行初等行变换和列变换，最终可以变换成对角形式，所以行秩必然等于列秩。


## 特征值分解

如果有向量${\bf x}$能使得矩阵${\bf A}$满足${\bf Ax} = \lambda {\bf x}$，那么$\lambda$和$x$就分别是${\bf A}$的特征值与特征向量。

特征值分解的意义在于对于 $A$ 这个线性变换操作来讲，可以找到对应地特征向量，使得 $A$ 这个线性变换操作当其特征向量时它只是将特征向量 ${\bf x}$ 数乘一个标量(对应的特征值$\lambda$)，相当于将 ${\bf x}$ 长度缩放 $\lambda$ 倍，因此对特征函数构成的空间中的任意一个函数的变换可以简化为各特征函数的加权和。

### 性质

实对称矩阵的特征向量一定正交。

## 奇异值分解 (SVD)

如果存在单位正交矩阵(标准正交基矢的转置就是自身的逆矩阵) $U$ 和 $V$，使得 $A = U \Sigma V^{T}$，$\Sigma$ 为对角矩阵，对角线上的值被称为奇异值$U$和$V$中的列分别被称为$A$的左奇异向量和右奇异向量。

矩阵的秩（Rank）等于奇异值分解（SVD）中的非零奇异值的数。对于任意$m \times n$矩阵$A$，其奇异值分解可以表示为。

$$
A = U \Sigma V^{T}
$$

其中：

* $U$是一个$m \times m$的正交矩阵

* $\Sigma$是一个$m \times n$的对角矩阵，对角线上的非负实数称为奇异值。

* $V$是一个$n \times n$的正交矩阵

奇异值衡量了矩阵$A$在其对应的奇异向量方向上的“拉伸”或“压缩”程度。非零奇异值表示了这种变换的显著性，而零奇异值则表示在对应方向上没有显著的变换。


## 特征值 VS 奇异值

### 定义对比

奇异值与特征值都被用于描述矩阵作用于某些向量的标量，都是描述向量模长变化幅度的数值。它们的差异在于：

* 特征向量描述的是矩阵的方向不变作用(invariant action)的向量；
* 奇异向量描述的是矩阵最大作用(maximum action)的方向向量。

这里的“作用”(action)所指的矩阵与向量的乘积得到一个新的向量，几何上相当于对向量进行了旋转和拉伸，就像是对向量施加了一个作用(action)，或者说是变换。

## 关联

奇异值是正交矩阵特征值的绝对值。

### 几何意义对比

参考  [奇异值与特征值辨析](https://zhuanlan.zhihu.com/p/353637184)

## PCA与SVD

PCA本质是基于SVD来做的，PCA的基本思想是降维度，先给一个结论，奇异值越大表明包含的信息越多，根据SVD我们有

$$
A = U \Sigma V^{T}
$$

在实际操作中，A为数据集($m=$样本数，$n=$特征数)，我们可以保留几个关键特征数实现降维，此时PCA干的事情可以写为

$$
{A_{m \times n}} \approx {U_{m \times r}}{\Sigma _{r \times r}}V_{r \times n}^T  \quad \quad r < n
$$

能这样做的原因是，在很多情况下，前$10\%$ 甚至 $1\%$ 的奇异值的和就占了全部的奇异值之和的$99\%$以上了。可以可以用前$r$大的奇异值来近似描述矩阵。右边的三个矩阵相乘的结果将会是一个接近于$A$的矩阵，在这儿，$r$越接近于$n$，则相乘的结果越接近于$A$。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵$A$，我们如果想要压缩空间来表示原矩阵$A$，我们存下这里的三个矩阵$U$、 $\Sigma$、 $V$就好了。

因此主成分分析实际上关注的是 ${ {\tilde A}_{m \times r} } = {A_{m \times n} }{V_{n \times r} } = {U_{m \times r} }{\Sigma _{r \times r} }$ 这个矩阵，它成功将特征数从$n$降低到了$r$。

# 不确定性分析


不确定性分析的目标在于：将模拟系统输入参数的不确定度传递到计算响应中，量化模拟系统计算响应的不确定度水平和不同响应之间的相关性信息。

# 敏感性分析

## Sobol指数敏感性分析

Sobol指数法是一种全局敏感性分析方法，基本原理是将模型分解为若干由单个参数或参数组合构成的函数，通过方差分解，得到各部分方差对于模型输出的总方差的贡献程度，进一步计算、分析模型中各变量的敏感度以及参数组合间的交互作用。

在 SALib 中，如果使用了 ProblemSpec 对象并调用了 analyze_sobol 方法进行敏感度分析，采样情况通常是在调用 sample 方法时确定的。ProblemSpec 对象会存储采样结果，并且可以使用它来进行后续的分析。